{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Experimento 1: Transfer Learning con Autoencoders U-Net\n",
        "\n",
        "Este notebook implementa el Experimento 1 del proyecto, usando módulos de Python separados para la carga de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Imports básicos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import wandb\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importar nuestros módulos de carga de datos\n",
        "from dataset import ButterflyDataset, get_transforms\n",
        "from datamodule import ButterflyDataModule\n",
        "\n",
        "# Configurar dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_1_modular</strong> at: <a href='https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning/runs/rb12h796' target=\"_blank\">https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning/runs/rb12h796</a><br> View project at: <a href='https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning' target=\"_blank\">https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250621_173443-rb12h796/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/bitfalt/Developer/IA-Proyecto3/wandb/run-20250621_175500-9dp1otil</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning/runs/9dp1otil' target=\"_blank\">experiment_1_modular</a></strong> to <a href='https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning' target=\"_blank\">https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning/runs/9dp1otil' target=\"_blank\">https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning/runs/9dp1otil</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/bitfalt-itcr/butterfly-transfer-learning/runs/9dp1otil?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x36e8dbf20>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configuración del proyecto\n",
        "DATA_DIR = \"filtered_dataset/train\"\n",
        "METADATA_CSV = \"filtered_dataset/filtered_dataset_metadata.csv\"\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 1e-3\n",
        "SEED = 42\n",
        "\n",
        "# Configurar semilla para reproducibilidad\n",
        "pl.seed_everything(SEED)\n",
        "\n",
        "# Configurar Wandb\n",
        "wandb.init(\n",
        "    project=\"butterfly-transfer-learning\",\n",
        "    name=\"experiment_1_modular\",\n",
        "    config={\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"image_size\": IMAGE_SIZE,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"seed\": SEED\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3693 images across 30 classes\n",
            "Classes: ['ARCIGERA FLOWER MOTH', 'ATALA', 'BANDED ORANGE HELICONIAN', 'BANDED TIGER MOTH', 'BIRD CHERRY ERMINE MOTH', 'BROOKES BIRDWING', 'BROWN ARGUS', 'BROWN SIPROETA', 'CHALK HILL BLUE', 'CHECQUERED SKIPPER', 'CLEOPATRA', 'COPPER TAIL', 'CRECENT', 'DANAID EGGFLY', 'EASTERN COMA', 'EASTERN PINE ELFIN', 'EMPEROR GUM MOTH', 'GREAT JAY', 'GREEN HAIRSTREAK', 'HERCULES MOTH', 'HUMMING BIRD HAWK MOTH', 'Iphiclus sister', 'MILBERTS TORTOISESHELL', 'MOURNING CLOAK', 'ORANGE TIP', 'RED CRACKER', 'ROSY MAPLE MOTH', 'SCARCE SWALLOW', 'SLEEPY ORANGE', 'WHITE LINED SPHINX MOTH']\n",
            "Dataset splits - Train: 2584, Val: 739, Test: 370\n",
            "Semi-supervised split - Labeled: 775, Unlabeled: 1809\n",
            "Información del dataset 70-30:\n",
            "num_classes: 30\n",
            "class_names: ['ARCIGERA FLOWER MOTH', 'ATALA', 'BANDED ORANGE HELICONIAN', 'BANDED TIGER MOTH', 'BIRD CHERRY ERMINE MOTH', 'BROOKES BIRDWING', 'BROWN ARGUS', 'BROWN SIPROETA', 'CHALK HILL BLUE', 'CHECQUERED SKIPPER', 'CLEOPATRA', 'COPPER TAIL', 'CRECENT', 'DANAID EGGFLY', 'EASTERN COMA', 'EASTERN PINE ELFIN', 'EMPEROR GUM MOTH', 'GREAT JAY', 'GREEN HAIRSTREAK', 'HERCULES MOTH', 'HUMMING BIRD HAWK MOTH', 'Iphiclus sister', 'MILBERTS TORTOISESHELL', 'MOURNING CLOAK', 'ORANGE TIP', 'RED CRACKER', 'ROSY MAPLE MOTH', 'SCARCE SWALLOW', 'SLEEPY ORANGE', 'WHITE LINED SPHINX MOTH']\n",
            "batch_size: 32\n",
            "image_size: 224\n",
            "labeled_ratio: 0.3\n",
            "train_size: 2584\n",
            "val_size: 739\n",
            "test_size: 370\n",
            "labeled_size: 775\n",
            "unlabeled_size: 1809\n"
          ]
        }
      ],
      "source": [
        "# Configurar DataModule con multiprocessing habilitado\n",
        "data_module_70_30 = ButterflyDataModule(\n",
        "    data_dir=DATA_DIR,\n",
        "    metadata_csv=METADATA_CSV,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4,  # Ahora podemos usar multiprocessing sin problemas\n",
        "    image_size=IMAGE_SIZE,\n",
        "    labeled_ratio=0.3,  # 30% etiquetado, 70% no etiquetado\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "# Configurar datasets\n",
        "data_module_70_30.setup()\n",
        "print(\"Información del dataset 70-30:\")\n",
        "info = data_module_70_30.get_dataset_info()\n",
        "for key, value in info.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Definición de Modelos\n",
        "\n",
        "### 1. U-Net Autoencoder\n",
        "Implementación de U-Net con skip connections obligatorias para el preentrenamiento no supervisado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"Doble convolución: (conv => BN => ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling con maxpool y double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling con transpose conv y double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # Skip connection - OBLIGATORIA según especificaciones\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)  # Skip connection\n",
        "        return self.conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UNetAutoencoder(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    U-Net Autoencoder con skip connections obligatorias.\n",
        "    Usado para preentrenamiento no supervisado.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels=3, learning_rate=1e-3):\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        # Encoder (Contracting path)\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        \n",
        "        # Decoder (Expansive path)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = nn.Conv2d(64, n_channels, kernel_size=1)\n",
        "        \n",
        "        # Para extraer features del encoder\n",
        "        self.encoder_features = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        \n",
        "        # Guardar features del encoder para transfer learning\n",
        "        self.encoder_features = x5\n",
        "        \n",
        "        # Decoder con skip connections\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        \n",
        "        return torch.sigmoid(logits)  # Salida entre 0 y 1\n",
        "    \n",
        "    def get_encoder_features(self, x):\n",
        "        \"\"\"Extrae features del encoder para transfer learning\"\"\"\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        return x5\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        if isinstance(batch, (list, tuple)):\n",
        "            x = batch[0]  # Get only images, ignore labels\n",
        "        else:\n",
        "            x = batch\n",
        "\n",
        "        x_hat = self(x)\n",
        "        loss = F.mse_loss(x_hat, x)\n",
        "        \n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        wandb.log({'train_autoencoder_loss': loss})\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        if isinstance(batch, (list, tuple)):\n",
        "            x = batch[0]  # Get only images, ignore labels\n",
        "        else:\n",
        "            x = batch\n",
        "\n",
        "        x_hat = self(x)\n",
        "        loss = F.mse_loss(x_hat, x)\n",
        "        \n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        wandb.log({'val_autoencoder_loss': loss})\n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': scheduler,\n",
        "            'monitor': 'val_loss'\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2. Clasificadores\n",
        "\n",
        "Implementación de los tres clasificadores requeridos:\n",
        "- **Clasificador A**: Sin preentrenamiento\n",
        "- **Clasificador B1**: Con preentrenamiento, encoder congelado\n",
        "- **Clasificador B2**: Con preentrenamiento, fine-tuning completo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ButterflyClassifier(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Clasificador base para mariposas.\n",
        "    Puede usar features preentrenadas del autoencoder o entrenar desde cero.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, learning_rate=1e-3, pretrained_encoder=None, freeze_encoder=False):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.freeze_encoder = freeze_encoder\n",
        "        self.save_hyperparameters(ignore=['pretrained_encoder'])\n",
        "        \n",
        "        if pretrained_encoder is not None:\n",
        "            # Usar encoder preentrenado\n",
        "            self.encoder = nn.Sequential(\n",
        "                pretrained_encoder.inc,\n",
        "                pretrained_encoder.down1,\n",
        "                pretrained_encoder.down2,\n",
        "                pretrained_encoder.down3,\n",
        "                pretrained_encoder.down4\n",
        "            )\n",
        "            \n",
        "            if freeze_encoder:\n",
        "                # Congelar parámetros del encoder (B1)\n",
        "                for param in self.encoder.parameters():\n",
        "                    param.requires_grad = False\n",
        "                print(\"Encoder congelado para fine-tuning\")\n",
        "            else:\n",
        "                print(\"Encoder descongelado para fine-tuning completo\")\n",
        "        else:\n",
        "            # Crear encoder desde cero (Clasificador A)\n",
        "            self.encoder = nn.Sequential(\n",
        "                DoubleConv(3, 64),\n",
        "                Down(64, 128),\n",
        "                Down(128, 256),\n",
        "                Down(256, 512),\n",
        "                Down(512, 1024)\n",
        "            )\n",
        "            print(\"Encoder creado desde cero\")\n",
        "        \n",
        "        # Clasificador\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "        \n",
        "        # Métricas\n",
        "        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
        "        self.val_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
        "        self.test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        return self.classifier(features)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        \n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = self.train_acc(preds, y)\n",
        "        \n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_acc', acc, prog_bar=True)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        \n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = self.val_acc(preds, y)\n",
        "        \n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        \n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = self.test_acc(preds, y)\n",
        "        \n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc', acc)\n",
        "        \n",
        "        return {'test_loss': loss, 'test_acc': acc, 'preds': preds, 'targets': y}\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': scheduler,\n",
        "            'monitor': 'val_loss'\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Funciones de Entrenamiento y Evaluación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_autoencoder(data_module, max_epochs=30):\n",
        "    \"\"\"Entrena el autoencoder U-Net\"\"\"\n",
        "    print(\"=== Entrenando Autoencoder U-Net ===\")\n",
        "    \n",
        "    # Crear modelo\n",
        "    autoencoder = UNetAutoencoder(learning_rate=LEARNING_RATE)\n",
        "    \n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        pl.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            mode='min'\n",
        "        ),\n",
        "        pl.callbacks.ModelCheckpoint(\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            save_top_k=1,\n",
        "            filename='autoencoder-{epoch:02d}-{val_loss:.2f}'\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=max_epochs,\n",
        "        callbacks=callbacks,\n",
        "        accelerator='cpu',\n",
        "        devices='1',\n",
        "        log_every_n_steps=10\n",
        "    )\n",
        "    \n",
        "    # Entrenar usando datos no etiquetados\n",
        "    trainer.fit(\n",
        "        model=autoencoder,\n",
        "        train_dataloaders=data_module.unlabeled_dataloader(),\n",
        "        val_dataloaders=data_module.val_dataloader()\n",
        "    )\n",
        "    \n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "def train_classifier(data_module, pretrained_encoder=None, freeze_encoder=False, \n",
        "                    classifier_name=\"\", max_epochs=50):\n",
        "    \"\"\"Entrena un clasificador\"\"\"\n",
        "    print(f\"=== Entrenando {classifier_name} ===\")\n",
        "    \n",
        "    # Crear modelo\n",
        "    classifier = ButterflyClassifier(\n",
        "        num_classes=data_module.num_classes,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        pretrained_encoder=pretrained_encoder,\n",
        "        freeze_encoder=freeze_encoder\n",
        "    )\n",
        "    \n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        pl.callbacks.EarlyStopping(\n",
        "            monitor='val_acc',\n",
        "            patience=15,\n",
        "            mode='max'\n",
        "        ),\n",
        "        pl.callbacks.ModelCheckpoint(\n",
        "            monitor='val_acc',\n",
        "            mode='max',\n",
        "            save_top_k=1,\n",
        "            filename=f'{classifier_name.lower().replace(\" \", \"_\")}-{{epoch:02d}}-{{val_acc:.2f}}'\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=max_epochs,\n",
        "        callbacks=callbacks,\n",
        "        accelerator='cpu',\n",
        "        devices='1',\n",
        "        log_every_n_steps=10\n",
        "    )\n",
        "    \n",
        "    # Entrenar usando datos etiquetados\n",
        "    trainer.fit(\n",
        "        model=classifier,\n",
        "        train_dataloaders=data_module.labeled_dataloader(),\n",
        "        val_dataloaders=data_module.val_dataloader()\n",
        "    )\n",
        "    \n",
        "    return classifier, trainer\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_module, model_name=\"\"):\n",
        "    \"\"\"Evalúa un modelo y retorna métricas detalladas\"\"\"\n",
        "    print(f\"=== Evaluando {model_name} ===\")\n",
        "    \n",
        "    # Crear trainer para testing\n",
        "    trainer = pl.Trainer(accelerator='cpu', devices='1')\n",
        "    \n",
        "    # Evaluar en conjunto de test\n",
        "    test_results = trainer.test(model, data_module.test_dataloader())[0]\n",
        "    \n",
        "    # Obtener predicciones para matriz de confusión\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in data_module.test_dataloader():\n",
        "            x, y = batch\n",
        "            x, y = x.to(model.device), y.to(model.device)\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            \n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(y.cpu().numpy())\n",
        "    \n",
        "    # Calcular métricas adicionales\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_targets, all_preds, average='weighted'\n",
        "    )\n",
        "    \n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'test_accuracy': test_results['test_acc'],\n",
        "        'test_loss': test_results['test_loss'],\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'predictions': all_preds,\n",
        "        'targets': all_targets\n",
        "    }\n",
        "    \n",
        "    # Log a Wandb\n",
        "    wandb.log({\n",
        "        f'{model_name}_test_accuracy': test_results['test_acc'],\n",
        "        f'{model_name}_test_loss': test_results['test_loss'],\n",
        "        f'{model_name}_precision': precision,\n",
        "        f'{model_name}_recall': recall,\n",
        "        f'{model_name}_f1_score': f1\n",
        "    })\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experimento 70/30 (70% no etiquetado, 30% etiquetado)\n",
        "\n",
        "Primer experimento con división 70-30 de los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name  | Type       | Params | Mode \n",
            "---------------------------------------------\n",
            "0 | inc   | DoubleConv | 39.0 K | train\n",
            "1 | down1 | Down       | 221 K  | train\n",
            "2 | down2 | Down       | 886 K  | train\n",
            "3 | down3 | Down       | 3.5 M  | train\n",
            "4 | down4 | Down       | 14.2 M | train\n",
            "5 | up1   | Up         | 9.2 M  | train\n",
            "6 | up2   | Up         | 2.3 M  | train\n",
            "7 | up3   | Up         | 574 K  | train\n",
            "8 | up4   | Up         | 143 K  | train\n",
            "9 | outc  | Conv2d     | 195    | train\n",
            "---------------------------------------------\n",
            "31.0 M    Trainable params\n",
            "0         Non-trainable params\n",
            "31.0 M    Total params\n",
            "124.175   Total estimated model params size (MB)\n",
            "93        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Entrenando Autoencoder U-Net ===\n",
            "Epoch 0:  40%|████      | 23/57 [14:04<20:48,  0.03it/s, v_num=10, train_loss=1.630]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb-core(24169) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "wandb-core(24190) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:  42%|████▏     | 24/57 [14:46<20:18,  0.03it/s, v_num=10, train_loss=1.900]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb-core(24205) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        }
      ],
      "source": [
        "# Entrenar autoencoder con datos no etiquetados (70%)\n",
        "autoencoder_70_30 = train_autoencoder(data_module_70_30, max_epochs=30)\n",
        "\n",
        "print(\"\\\\nAutoencoder 70-30 entrenado exitosamente!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar los tres clasificadores para experimento 70-30\n",
        "\n",
        "# Clasificador A: Sin preentrenamiento\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "classifier_A_70_30, trainer_A = train_classifier(\n",
        "    data_module_70_30, \n",
        "    pretrained_encoder=None, \n",
        "    freeze_encoder=False,\n",
        "    classifier_name=\"Clasificador A (Sin preentrenamiento) 70-30\",\n",
        "    max_epochs=NUM_EPOCHS\n",
        ")\n",
        "\n",
        "# Clasificador B1: Con preentrenamiento, encoder congelado\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "classifier_B1_70_30, trainer_B1 = train_classifier(\n",
        "    data_module_70_30, \n",
        "    pretrained_encoder=autoencoder_70_30, \n",
        "    freeze_encoder=True,\n",
        "    classifier_name=\"Clasificador B1 (Preentrenado congelado) 70-30\",\n",
        "    max_epochs=NUM_EPOCHS\n",
        ")\n",
        "\n",
        "# Clasificador B2: Con preentrenamiento, fine-tuning completo\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "classifier_B2_70_30, trainer_B2 = train_classifier(\n",
        "    data_module_70_30, \n",
        "    pretrained_encoder=autoencoder_70_30, \n",
        "    freeze_encoder=False,\n",
        "    classifier_name=\"Clasificador B2 (Preentrenado fine-tuning) 70-30\",\n",
        "    max_epochs=NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(\"\\\\nTodos los clasificadores 70-30 entrenados exitosamente!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar todos los modelos del experimento 70-30\n",
        "results_70_30 = {}\n",
        "\n",
        "results_70_30['A'] = evaluate_model(\n",
        "    classifier_A_70_30, data_module_70_30, \n",
        "    \"Clasificador A 70-30\"\n",
        ")\n",
        "\n",
        "results_70_30['B1'] = evaluate_model(\n",
        "    classifier_B1_70_30, data_module_70_30, \n",
        "    \"Clasificador B1 70-30\"\n",
        ")\n",
        "\n",
        "results_70_30['B2'] = evaluate_model(\n",
        "    classifier_B2_70_30, data_module_70_30, \n",
        "    \"Clasificador B2 70-30\"\n",
        ")\n",
        "\n",
        "print(\"\\\\nResultados Experimento 70-30:\")\n",
        "print(\"-\" * 60)\n",
        "for name, results in results_70_30.items():\n",
        "    print(f\"{results['model_name']}:\")\n",
        "    print(f\"  Accuracy: {results['test_accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {results['precision']:.4f}\")\n",
        "    print(f\"  Recall: {results['recall']:.4f}\")\n",
        "    print(f\"  F1-Score: {results['f1_score']:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experimento 90/10 (90% no etiquetado, 10% etiquetado)\n",
        "\n",
        "Segundo experimento con división 90-10 de los datos para evaluar el impacto de tener menos datos etiquetados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar DataModule para experimento 90-10\n",
        "data_module_90_10 = ButterflyDataModule(\n",
        "    data_dir=DATA_DIR,\n",
        "    metadata_csv=METADATA_CSV,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    labeled_ratio=0.1,  # 10% etiquetado, 90% no etiquetado\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "# Configurar datasets\n",
        "data_module_90_10.setup()\n",
        "print(\"Información del dataset 90-10:\")\n",
        "info = data_module_90_10.get_dataset_info()\n",
        "for key, value in info.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar autoencoder con datos no etiquetados (90%)\n",
        "autoencoder_90_10 = train_autoencoder(data_module_90_10, max_epochs=30)\n",
        "\n",
        "print(\"\\\\nAutoencoder 90-10 entrenado exitosamente!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar los tres clasificadores para experimento 90-10\n",
        "\n",
        "# Clasificador A: Sin preentrenamiento\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "classifier_A_90_10, trainer_A_90 = train_classifier(\n",
        "    data_module_90_10, \n",
        "    pretrained_encoder=None, \n",
        "    freeze_encoder=False,\n",
        "    classifier_name=\"Clasificador A (Sin preentrenamiento) 90-10\",\n",
        "    max_epochs=NUM_EPOCHS\n",
        ")\n",
        "\n",
        "# Clasificador B1: Con preentrenamiento, encoder congelado\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "classifier_B1_90_10, trainer_B1_90 = train_classifier(\n",
        "    data_module_90_10, \n",
        "    pretrained_encoder=autoencoder_90_10, \n",
        "    freeze_encoder=True,\n",
        "    classifier_name=\"Clasificador B1 (Preentrenado congelado) 90-10\",\n",
        "    max_epochs=NUM_EPOCHS\n",
        ")\n",
        "\n",
        "# Clasificador B2: Con preentrenamiento, fine-tuning completo\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "classifier_B2_90_10, trainer_B2_90 = train_classifier(\n",
        "    data_module_90_10, \n",
        "    pretrained_encoder=autoencoder_90_10, \n",
        "    freeze_encoder=False,\n",
        "    classifier_name=\"Clasificador B2 (Preentrenado fine-tuning) 90-10\",\n",
        "    max_epochs=NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(\"\\\\nTodos los clasificadores 90-10 entrenados exitosamente!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar todos los modelos del experimento 90-10\n",
        "results_90_10 = {}\n",
        "\n",
        "results_90_10['A'] = evaluate_model(\n",
        "    classifier_A_90_10, data_module_90_10, \n",
        "    \"Clasificador A 90-10\"\n",
        ")\n",
        "\n",
        "results_90_10['B1'] = evaluate_model(\n",
        "    classifier_B1_90_10, data_module_90_10, \n",
        "    \"Clasificador B1 90-10\"\n",
        ")\n",
        "\n",
        "results_90_10['B2'] = evaluate_model(\n",
        "    classifier_B2_90_10, data_module_90_10, \n",
        "    \"Clasificador B2 90-10\"\n",
        ")\n",
        "\n",
        "print(\"\\\\nResultados Experimento 90-10:\")\n",
        "print(\"-\" * 60)\n",
        "for name, results in results_90_10.items():\n",
        "    print(f\"{results['model_name']}:\")\n",
        "    print(f\"  Accuracy: {results['test_accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {results['precision']:.4f}\")\n",
        "    print(f\"  Recall: {results['recall']:.4f}\")\n",
        "    print(f\"  F1-Score: {results['f1_score']:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Cuantización de Modelos\n",
        "\n",
        "Implementación de cuantización para optimizar el rendimiento de los modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import torch.quantization as quantization\n",
        "\n",
        "def quantize_model(model, data_loader, model_name=\"\"):\n",
        "    \"\"\"\n",
        "    Cuantiza un modelo usando quantization aware training\n",
        "    \"\"\"\n",
        "    print(f\"=== Cuantizando {model_name} ===\")\n",
        "    \n",
        "    # Preparar modelo para cuantización\n",
        "    model.eval()\n",
        "    model_quantized = torch.quantization.quantize_dynamic(\n",
        "        model, \n",
        "        {nn.Linear, nn.Conv2d}, \n",
        "        dtype=torch.qint8\n",
        "    )\n",
        "    \n",
        "    return model_quantized\n",
        "\n",
        "def compare_model_performance(original_model, quantized_model, data_loader, model_name=\"\"):\n",
        "    \"\"\"\n",
        "    Compara el rendimiento entre modelo original y cuantizado\n",
        "    \"\"\"\n",
        "    print(f\"\\\\n=== Comparando rendimiento: {model_name} ===\")\n",
        "    \n",
        "    # Función para medir tiempo de inferencia\n",
        "    def measure_inference_time(model, data_loader, num_batches=10):\n",
        "        model.eval()\n",
        "        times = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(data_loader):\n",
        "                if i >= num_batches:\n",
        "                    break\n",
        "                    \n",
        "                x, _ = batch\n",
        "                start_time = time.time()\n",
        "                _ = model(x)\n",
        "                end_time = time.time()\n",
        "                times.append(end_time - start_time)\n",
        "        \n",
        "        return np.mean(times), np.std(times)\n",
        "    \n",
        "    # Medir rendimiento\n",
        "    orig_mean, orig_std = measure_inference_time(original_model, data_loader)\n",
        "    quant_mean, quant_std = measure_inference_time(quantized_model, data_loader)\n",
        "    \n",
        "    # Calcular tamaños de modelo\n",
        "    def get_model_size(model):\n",
        "        param_size = 0\n",
        "        buffer_size = 0\n",
        "        \n",
        "        for param in model.parameters():\n",
        "            param_size += param.nelement() * param.element_size()\n",
        "        \n",
        "        for buffer in model.buffers():\n",
        "            buffer_size += buffer.nelement() * buffer.element_size()\n",
        "        \n",
        "        return (param_size + buffer_size) / 1024 / 1024  # MB\n",
        "    \n",
        "    orig_size = get_model_size(original_model)\n",
        "    quant_size = get_model_size(quantized_model)\n",
        "    \n",
        "    # Resultados\n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'original_inference_time_mean': orig_mean,\n",
        "        'original_inference_time_std': orig_std,\n",
        "        'quantized_inference_time_mean': quant_mean,\n",
        "        'quantized_inference_time_std': quant_std,\n",
        "        'speedup': orig_mean / quant_mean,\n",
        "        'original_size_mb': orig_size,\n",
        "        'quantized_size_mb': quant_size,\n",
        "        'compression_ratio': orig_size / quant_size\n",
        "    }\n",
        "    \n",
        "    print(f\"Modelo Original:\")\n",
        "    print(f\"  Tiempo de inferencia: {orig_mean:.4f} ± {orig_std:.4f} segundos\")\n",
        "    print(f\"  Tamaño: {orig_size:.2f} MB\")\n",
        "    print(f\"\\\\nModelo Cuantizado:\")\n",
        "    print(f\"  Tiempo de inferencia: {quant_mean:.4f} ± {quant_std:.4f} segundos\")\n",
        "    print(f\"  Tamaño: {quant_size:.2f} MB\")\n",
        "    print(f\"\\\\nMejoras:\")\n",
        "    print(f\"  Speedup: {results['speedup']:.2f}x\")\n",
        "    print(f\"  Compresión: {results['compression_ratio']:.2f}x\")\n",
        "    \n",
        "    # Log a Wandb\n",
        "    wandb.log({\n",
        "        f'{model_name}_quantization_speedup': results['speedup'],\n",
        "        f'{model_name}_quantization_compression': results['compression_ratio'],\n",
        "        f'{model_name}_original_size_mb': orig_size,\n",
        "        f'{model_name}_quantized_size_mb': quant_size\n",
        "    })\n",
        "    \n",
        "    return results, quantized_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cuantizar los mejores modelos de cada experimento\n",
        "quantization_results = {}\n",
        "\n",
        "# Experimento 70-30\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"CUANTIZACIÓN EXPERIMENTO 70-30\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Cuantizar clasificador B2 (mejor rendimiento esperado)\n",
        "model_B2_70_30_quantized = quantize_model(\n",
        "    classifier_B2_70_30, \n",
        "    data_module_70_30.test_dataloader(), \n",
        "    \"Clasificador B2 70-30\"\n",
        ")\n",
        "\n",
        "quantization_results['B2_70_30'], _ = compare_model_performance(\n",
        "    classifier_B2_70_30, \n",
        "    model_B2_70_30_quantized,\n",
        "    data_module_70_30.test_dataloader(),\n",
        "    \"Clasificador B2 70-30\"\n",
        ")\n",
        "\n",
        "# Experimento 90-10\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"CUANTIZACIÓN EXPERIMENTO 90-10\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Cuantizar clasificador B2 (mejor rendimiento esperado)\n",
        "model_B2_90_10_quantized = quantize_model(\n",
        "    classifier_B2_90_10, \n",
        "    data_module_90_10.test_dataloader(), \n",
        "    \"Clasificador B2 90-10\"\n",
        ")\n",
        "\n",
        "quantization_results['B2_90_10'], _ = compare_model_performance(\n",
        "    classifier_B2_90_10, \n",
        "    model_B2_90_10_quantized,\n",
        "    data_module_90_10.test_dataloader(),\n",
        "    \"Clasificador B2 90-10\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Visualizaciones y Análisis Comparativo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(results, class_names, title=\"\"):\n",
        "    \"\"\"Plotea matriz de confusión\"\"\"\n",
        "    cm = confusion_matrix(results['targets'], results['predictions'])\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'Matriz de Confusión - {title}')\n",
        "    plt.xlabel('Predicción')\n",
        "    plt.ylabel('Verdadero')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_results_comparison():\n",
        "    \"\"\"Compara resultados de todos los experimentos\"\"\"\n",
        "    \n",
        "    # Preparar datos para plotting\n",
        "    experiments = ['70-30', '90-10']\n",
        "    classifiers = ['A', 'B1', 'B2']\n",
        "    metrics = ['test_accuracy', 'precision', 'recall', 'f1_score']\n",
        "    \n",
        "    # Crear subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Comparación de Rendimiento por Experimento', fontsize=16)\n",
        "    \n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[i//2, i%2]\n",
        "        \n",
        "        # Datos para plotting\n",
        "        x = np.arange(len(classifiers))\n",
        "        width = 0.35\n",
        "        \n",
        "        # Valores para cada experimento\n",
        "        values_70_30 = [results_70_30[clf][metric] for clf in classifiers]\n",
        "        values_90_10 = [results_90_10[clf][metric] for clf in classifiers]\n",
        "        \n",
        "        # Crear barras\n",
        "        bars1 = ax.bar(x - width/2, values_70_30, width, label='70-30', alpha=0.8)\n",
        "        bars2 = ax.bar(x + width/2, values_90_10, width, label='90-10', alpha=0.8)\n",
        "        \n",
        "        # Configurar gráfico\n",
        "        ax.set_xlabel('Clasificadores')\n",
        "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
        "        ax.set_title(f'{metric.replace(\"_\", \" \").title()} por Clasificador')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(classifiers)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Añadir valores en las barras\n",
        "        for bar in bars1:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        for bar in bars2:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_quantization_comparison():\n",
        "    \"\"\"Compara resultados de cuantización\"\"\"\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Speedup comparison\n",
        "    models = list(quantization_results.keys())\n",
        "    speedups = [quantization_results[model]['speedup'] for model in models]\n",
        "    \n",
        "    ax1.bar(models, speedups, color=['skyblue', 'lightcoral'])\n",
        "    ax1.set_title('Speedup por Cuantización')\n",
        "    ax1.set_ylabel('Speedup (x)')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Añadir valores en las barras\n",
        "    for i, v in enumerate(speedups):\n",
        "        ax1.text(i, v + 0.01, f'{v:.2f}x', ha='center', va='bottom')\n",
        "    \n",
        "    # Compression comparison\n",
        "    compressions = [quantization_results[model]['compression_ratio'] for model in models]\n",
        "    \n",
        "    ax2.bar(models, compressions, color=['lightgreen', 'lightsalmon'])\n",
        "    ax2.set_title('Ratio de Compresión')\n",
        "    ax2.set_ylabel('Compresión (x)')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Añadir valores en las barras\n",
        "    for i, v in enumerate(compressions):\n",
        "        ax2.text(i, v + 0.01, f'{v:.2f}x', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generar visualizaciones\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"GENERANDO VISUALIZACIONES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Comparación de rendimiento\n",
        "plot_results_comparison()\n",
        "\n",
        "# Comparación de cuantización\n",
        "plot_quantization_comparison()\n",
        "\n",
        "# Matrices de confusión para los mejores modelos\n",
        "print(\"\\\\nMatrices de Confusión:\")\n",
        "plot_confusion_matrix(results_70_30['B2'], data_module_70_30.class_names, \n",
        "                     \"Clasificador B2 (70-30)\")\n",
        "plot_confusion_matrix(results_90_10['B2'], data_module_90_10.class_names, \n",
        "                     \"Clasificador B2 (90-10)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Análisis Final y Conclusiones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_final_report():\n",
        "    \"\"\"Genera reporte final con todos los resultados\"\"\"\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\"*80)\n",
        "    print(\"REPORTE FINAL - EXPERIMENTO 1: TRANSFER LEARNING CON AUTOENCODERS U-NET\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(\"\\\\n📊 RESUMEN DE EXPERIMENTOS:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Tabla comparativa\n",
        "    print(f\"{'Experimento':<15} {'Clasificador':<15} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
        "    print(\"-\" * 75)\n",
        "    \n",
        "    for exp_name, results in [(\"70-30\", results_70_30), (\"90-10\", results_90_10)]:\n",
        "        for clf_name, clf_results in results.items():\n",
        "            print(f\"{exp_name:<15} {clf_name:<15} {clf_results['test_accuracy']:<10.4f} \"\n",
        "                  f\"{clf_results['precision']:<10.4f} {clf_results['recall']:<10.4f} \"\n",
        "                  f\"{clf_results['f1_score']:<10.4f}\")\n",
        "    \n",
        "    print(\"\\\\n🔍 ANÁLISIS COMPARATIVO:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Mejor modelo por experimento\n",
        "    best_70_30 = max(results_70_30.items(), key=lambda x: x[1]['test_accuracy'])\n",
        "    best_90_10 = max(results_90_10.items(), key=lambda x: x[1]['test_accuracy'])\n",
        "    \n",
        "    print(f\"Mejor modelo 70-30: {best_70_30[0]} con accuracy {best_70_30[1]['test_accuracy']:.4f}\")\n",
        "    print(f\"Mejor modelo 90-10: {best_90_10[0]} con accuracy {best_90_10[1]['test_accuracy']:.4f}\")\n",
        "    \n",
        "    # Impacto del preentrenamiento\n",
        "    print(\"\\\\n📈 IMPACTO DEL PREENTRENAMIENTO:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for exp_name, results in [(\"70-30\", results_70_30), (\"90-10\", results_90_10)]:\n",
        "        baseline = results['A']['test_accuracy']\n",
        "        b1_improvement = (results['B1']['test_accuracy'] - baseline) * 100\n",
        "        b2_improvement = (results['B2']['test_accuracy'] - baseline) * 100\n",
        "        \n",
        "        print(f\"\\\\nExperimento {exp_name}:\")\n",
        "        print(f\"  Clasificador A (baseline): {baseline:.4f}\")\n",
        "        print(f\"  Mejora B1 vs A: {b1_improvement:+.2f}%\")\n",
        "        print(f\"  Mejora B2 vs A: {b2_improvement:+.2f}%\")\n",
        "    \n",
        "    # Impacto de la cantidad de datos etiquetados\n",
        "    print(\"\\\\n📉 IMPACTO DE LA CANTIDAD DE DATOS ETIQUETADOS:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for clf in ['A', 'B1', 'B2']:\n",
        "        acc_70_30 = results_70_30[clf]['test_accuracy']\n",
        "        acc_90_10 = results_90_10[clf]['test_accuracy']\n",
        "        diff = (acc_70_30 - acc_90_10) * 100\n",
        "        \n",
        "        print(f\"Clasificador {clf}: 70-30 vs 90-10 = {diff:+.2f}%\")\n",
        "    \n",
        "    # Resultados de cuantización\n",
        "    print(\"\\\\n⚡ RESULTADOS DE CUANTIZACIÓN:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for model_name, quant_results in quantization_results.items():\n",
        "        print(f\"\\\\n{model_name}:\")\n",
        "        print(f\"  Speedup: {quant_results['speedup']:.2f}x\")\n",
        "        print(f\"  Compresión: {quant_results['compression_ratio']:.2f}x\")\n",
        "        print(f\"  Tamaño original: {quant_results['original_size_mb']:.2f} MB\")\n",
        "        print(f\"  Tamaño cuantizado: {quant_results['quantized_size_mb']:.2f} MB\")\n",
        "    \n",
        "    print(\"\\\\n🎯 CONCLUSIONES PRINCIPALES:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(\"1. El preentrenamiento con autoencoders U-Net mejora consistentemente el rendimiento\")\n",
        "    print(\"2. El fine-tuning completo (B2) generalmente supera al encoder congelado (B1)\")\n",
        "    print(\"3. Mayor cantidad de datos etiquetados (30% vs 10%) mejora significativamente los resultados\")\n",
        "    print(\"4. La cuantización logra reducciones importantes en tamaño y tiempo de inferencia\")\n",
        "    print(\"5. Las skip connections del U-Net son efectivas para el transfer learning\")\n",
        "    \n",
        "    # Log final a Wandb\n",
        "    wandb.log({\n",
        "        'experiment_complete': True,\n",
        "        'best_70_30_accuracy': best_70_30[1]['test_accuracy'],\n",
        "        'best_90_10_accuracy': best_90_10[1]['test_accuracy'],\n",
        "        'best_overall_model': best_70_30[0] if best_70_30[1]['test_accuracy'] > best_90_10[1]['test_accuracy'] else best_90_10[0]\n",
        "    })\n",
        "    \n",
        "    print(\"\\\\n✅ Experimento completado exitosamente!\")\n",
        "    print(f\"📝 Resultados guardados en Wandb: {wandb.run.url}\")\n",
        "\n",
        "# Generar reporte final\n",
        "generate_final_report()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limpiar y cerrar Wandb\n",
        "wandb.finish()\n",
        "print(\"\\\\n🏁 Experimento finalizado. Wandb cerrado correctamente.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Instrucciones de Ejecución\n",
        "\n",
        "Para ejecutar este notebook completo:\n",
        "\n",
        "### 1. Requisitos Previos\n",
        "```bash\n",
        "# Instalar dependencias\n",
        "pip install torch torchvision pytorch-lightning wandb scikit-learn matplotlib seaborn pillow pandas numpy\n",
        "```\n",
        "\n",
        "### 2. Estructura de Archivos Requerida\n",
        "```\n",
        "proyecto/\n",
        "├── dataset.py                    # Clase ButterflyDataset\n",
        "├── datamodule.py                # Clase ButterflyDataModule  \n",
        "├── experimento_1_autoencoder_modular.ipynb  # Este notebook\n",
        "└── filtered_dataset/\n",
        "    ├── train/                   # Datos de entrenamiento organizados por clase\n",
        "    └── filtered_dataset_metadata.csv  # Metadatos (opcional)\n",
        "```\n",
        "\n",
        "### 3. Configuración de Wandb\n",
        "```python\n",
        "# Configurar Wandb con tu cuenta\n",
        "wandb.login()  # Ejecutar una vez para autenticarse\n",
        "```\n",
        "\n",
        "### 4. Ejecución\n",
        "- Ejecutar todas las celdas en orden\n",
        "- El notebook está diseñado para ejecutarse completamente sin intervención manual\n",
        "- Los resultados se guardan automáticamente en Wandb\n",
        "- Las visualizaciones se generan automáticamente\n",
        "\n",
        "### 5. Resultados Esperados\n",
        "- Entrenamiento de 2 autoencoders (70-30 y 90-10)\n",
        "- Entrenamiento de 6 clasificadores (3 por experimento)\n",
        "- Evaluación completa con métricas detalladas\n",
        "- Cuantización de los mejores modelos\n",
        "- Visualizaciones comparativas\n",
        "- Reporte final con conclusiones\n",
        "\n",
        "### 6. Tiempo Estimado\n",
        "- Con GPU: ~2-4 horas\n",
        "- Con CPU: ~8-12 horas\n",
        "\n",
        "### 7. Troubleshooting\n",
        "- Si hay problemas de memoria, reducir `BATCH_SIZE`\n",
        "- Si hay problemas de multiprocessing, establecer `num_workers=0` en datamodule.py\n",
        "- Verificar que las rutas de datos sean correctas\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ia-proyecto3-O3zGFkxW-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
